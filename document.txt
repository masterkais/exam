import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset=pd.read_csv('Data.csv')
X=dataset.iloc[:,:-1].values
Y=dataset.iloc[:,3].values


# Taking Care of Missing Data
from sklearn.preprocessing import Imputer
imputer=Imputer(missing_values='NaN',strategy='mean',axis=0)
imputer=imputer.fit(X[:,1:3])
X[:,1:3]=imputer.transform(X[:,1:3])

# Encoding the Categorical Data
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelEncoder_X=LabelEncoder()
X[:,0]=labelEncoder_X.fit_transform(X[:,0])
onehotencoder=OneHotEncoder(categorical_features=[0])
X=onehotencoder.fit_transform(X).toarray()
labelencoder_Y=LabelEncoder()
Y=labelencoder_Y.fit_transform(Y)

from sklearn.compose import ColumnTransformer 
ct = ColumnTransformer([("State", OneHotEncoder(), [3])], remainder = 'passthrough‚Äô)     
x = ct.fit_transform(x) 

# Splitting the dataset into Training Set and Test Set
from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
X_train=sc.fit_transform(X_train)
X_test=sc.fit_transform(X_test)

#dessiner le scatter pour visualiser les donner
import matplotlib.pyplot as plt
import pandas as pd
dataset=pd.read_csv(‚ÄòSalary_Data‚Äô)
X=dataset.iloc[:,:-1]
Y=dataset.iloc[:,-1]
plt.scatter(X,y,color='red')
plt.title('Salary vs Experience')
plt.xlabel('Years of experience')
plt.ylabel('Salary')
plt.show()


#appliquer le model regression lin√©aire
from sklearn.linear_model import LinearRegression
regressor=LinearRegression() 
regressor.fit(X_train,y_train)

#prediction 
y_pred=regressor.predict(X_test)


#dessiner le model avec scatter 
plt.scatter(X_train,y_train,color=‚Äòred‚Äô)
plt.plot(X_train,regressor.predict(X_train),color=‚Äòblue‚Äô)
plt.title(‚ÄòSalary vs experience‚Äô) 
plt.xlabel(‚ÄòExperience‚Äô)
plt.ylabel(‚ÄòSalary‚Äô) 
plt.show()

#pour calculer les erreures de prediction
from sklearn import metrics
MAE= metrics.mean_absolute_error(Y_pred,y_test)
MSE= metrics.mean_squared_error(Y_pred,y_test) 
RMSE= metrics.mean_squared_error(Y_pred,y_test)**0.5

#coeffecent de correleation
#The correlation coefficient is the specific measure that quantifies the strength of the linear relationship between two variables in a correlation analysis
df_females.corr()


#appliquer le model multiliear regression et calculer le score et le coeffecient et l'intercept
from sklearn.linear_model import LinearRegression
regressor=LinearRegression()
regressor.fit(x_train,y_train)
Y_pred=regressor.predict(U)
from sklearn.metrics import r2_score
r2=r2_score(Y, Y_pred)


#Backward Elimination
    #STEP 1: Select a significance level to stay in the model (e.g. SL=0.05)
    #STEP 2: Fit the full model with all possible predictors
    #STEP 3: Consider the predictor with highest P-value. If P> SL, go to STEP 4, 
    #STEP 4: Remove the predictor
    #STEP 5: Fit the model without this variable.
#Building the Optimal Model Using Backward Elimination
import statsmodels.formula.api as sm
X=np.append(arr=np.ones((50,1)).astype(int),values=X,axis=1)
X_opt=X[:,[0,1,2,3,4,5]] 
regressor_OLS=sm.OLS(Y,X_opt).fit()     
regressor_OLS.summary()


#Forward Selection
   #STEP 1: Select a significance level to enter the model (e.g. SL=0.05=5%)
   #STEP 2: Fit all simple regression model (Y~ X_ùëõ). Select the one with the lowest P-value
   #STEP 3: Keep this variable and fit all possible models with one extra predictor added to the one(s) you already have
   #STEP 4: Consider the predictor with the lowest P-value. If P < SL go to STEP 3, 
#Bidirectional Elimination (Stepwise Regression)
 #    STEP 1: Select a significance level to enter and to stay in the model (e.g. SLENTER=0.05=5% and SLSTAY= 0.05=5%)
 #    STEP 2: Perform the next step of Forward Selection (new variable must have P < SLENTER to enter)
#     STEP 3: Perform all steps of Backward Elimination (variables must have P< SLSTAY to stay)  
#    STEP 4: No new variables can enter and no old variables can exit

# Polynomial Regression
#Building and Visualizing the results
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
poly_reg=PolynomialFeatures(degree=2)
X_poly=poly_reg.fit_transform(X)
regressor=LinearRegression()
regressor.fit(X_poly,Y)
plt.scatter(X,Y,color='red')
plt.plot(X, regressor.predict(X_poly),color='blue')
plt.title('Salary vs Level (Polynomial Regression)')
plt.xlabel('Level')
plt.ylabel('Salary')
plt.show() 
# Coefficients and error
from sklearn.metrics imort mean_squared_error
Constant=regressor.intercept_
Coefficients=regressor.coef_
Y_pred=regressor.predict(X_poly)
MAE= metrics.mean_absolute_error(Y_pred, Y)
MSE= metrics.mean_squared_error(Y_pred, Y)
RMSE= metrics.mean_squarred_error(Y_pred,Y)**0.5
    
#Fitting svm/SVR to the Dataset
from sklearn.svm import SVR
regressor=SVR(kernel='rbf')
regressor.fit(X ,Y)
#Prediction a New result 
Z_pred=regressor.predict(X)
Y_pred=regressor.predict(6.5) 


#Decision Tree Regression
from sklearn.tree import DecisionTreeRegressor
regressor=DecisionTreeRegressor(n_estimators = 10,random_state=0)
regressor.fit(X ,Y)


# Visualizing the Decision Tree Regression Results 
X_grid=np.arange(min(X),max(X),0.1)
X_grid=X_grid.reshape((len(X_grid),1))
plt.scatter(X,Y,Color= 'magenta')
plt.plot(X_grid,regressor.predict(X_grid),color='green')
plt.title('Truth or bluff (Decision Tree Regression')
plt.xlabel('Position label')
plt.ylabel('Salary')
plt.show()

#Visualizing the Decision Tree Regression Results (for higher resolution and smoother curve)
X_grid=np.arange(min(X),max(X),0.01)
X_grid=X_grid.reshape((len(X_grid),1))
plt.scatter(X,Y,Color= 'magenta')
plt.plot(X_grid,regressor.predict(X_grid),color='black')
plt.title('Truth or bluff (Decision Tree Regression')
plt.xlabel('Position label')
plt.ylabel('Salary')
plt.show()


#Random Forest Regression
# Fitting Random Forest Regression to the Dataset
from sklearn.ensemble import RandomForestRegressor
regressor = RandomForestRegressor(n_estimators = 10, random_state=0)
regressor.fit(X ,Y)


**************classification

#Using the elbow method to find the optimal number of clusters
from sklearn.cluster import KMeans
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)
plt.plot(range(1, 11), wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

# Fitting K-Means to the dataset
kmeans = KMeans(n_clusters = 5, init = 'k-means++', random_state = 42)
y_kmeans = kmeans.fit_predict(X)


# Visualising the clusters
plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Cluster 1')
plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')
plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Cluster 3')
plt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')
plt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')
plt.title('Clusters of customers')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()
plt.show()

#Agglomerative Hierarchical Clustering
from sklearn.cluster import AgglomerativeClustering
hc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward')y_hc = hc.fit_predict(X)

#Cluster 1¬†- More Income & Less Spending - Careful ClientsCluster 2¬†- Average Income & Average Spending ¬†- Standard ClientsCluster 3¬†- High Income & High Spending - Potential ClientsCluster 4¬†- Less Income & High Spending - Careless ClientsCluster 5¬†- Less Income & Less Spending ¬†- Sensible Clients


# Fitting Logistic Regression to the Training set
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state = 0)
classifier.fit(X_train, y_train)


#8) Visualising the Training set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
               np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
                     alpha = 0.4, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
      plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                       c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Logistic Regression (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()


#fitting svm 
from sklearn.svm import SVC
classifier = SVC(kernel = 'linear', random_state = 0)
classifier.fit(X_train, y_train)

#Predicting the Test set results
y_pred = classifier.predict(X_test)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)



#Fitting Kernel SVM to the Training set
from sklearn.svm import SVC
classifier = SVC(kernel = 'rbf', random_state = 0)
classifier.fit(X_train, y_train)
6) Predicting the Test set results
y_pred = classifier.predict(X_test)


# make a classifier of KNN algorithm
from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p =2)
 classifier.fit(X_train, y_train)
classifier.predict(x_test)

#from sklearn.naive_bayes import GaussianNB
classifier=GaussianNB()
classifier.fit(X_train,Y_train)


#5) Fitting Decision Tree Classification to the Training set
from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifier.fit(X_train, y_train)







